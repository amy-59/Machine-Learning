{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653639ea",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Batch Gradient Descent </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35060db9",
   "metadata": {},
   "source": [
    "Let's say there are a total of __m__ observations in a data set and we use __all these observations to calculate__ the cost function J, then this is known as <code>Batch Gradient Descent.</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80fc1d7",
   "metadata": {},
   "source": [
    "### Advantage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6443436",
   "metadata": {},
   "source": [
    "1. Batch Gradient Descent is great for convex or relatively smooth error manifolds.\n",
    "2. Batch gradient descent is more accurate since it computes the gradient using the entire training dataset.\n",
    "3. Also, Batch GD scales well with the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab76f57",
   "metadata": {},
   "source": [
    "### Disadvantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d94c4b",
   "metadata": {},
   "source": [
    "1. Batch Gradient Descent involves calculations over the full training set at each step as a result of which it is __very slow__ on very large training data. \n",
    "2. Computationally expensive.\n",
    "3. For Non-convex function it can get stuck in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919ebec",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffc7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6252f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cacece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82af136d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b296c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1947d",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5f2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd42b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baaf96",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eddcbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b060cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72be2179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -9.16088483, -205.46225988,  516.68462383,  340.62734108,\n",
       "       -895.54360867,  561.21453306,  153.88478595,  126.73431596,\n",
       "        861.12139955,   52.41982836])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2b24f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.88334520854633"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59733480",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "163238aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadf59ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score =  0.4399387660024645\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"r2_score = \", r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f67474",
   "metadata": {},
   "source": [
    "### Custom Class for Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a98623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37fbd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    \n",
    "    def __init__ (self, learning_rate=0.01, epochs= 100):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs =epochs\n",
    "        \n",
    "    def fit(self,X_train, y_train):\n",
    "        #initialize parameters : b=0, b1,b2,---bn = 1\n",
    "        self.intercept_ = 0 \n",
    "        \n",
    "        #X_train shape will tell how many coef are there\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # Update all coef_ and intercept_\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_  #y_hat = b + wT.x\n",
    "            \n",
    "            intercept_derv = - 2 * np.mean(y_train-y_hat) # dl/db = -2 * sum( y-y_hat )\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_derv) # bnew = bold - n*slop\n",
    "            \n",
    "            #update Coef\n",
    "            coef_derv = -2* np.dot((y_train- y_hat),X_train) #dl/dm = -2 *sum( ( y-y_hat ) * X)\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_derv)  # mnew = mold - n*slop\n",
    "            \n",
    "            \n",
    "        #print(y_hat.shape)\n",
    "        print(self.intercept_, self.coef_)\n",
    "        \n",
    "        \n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        return np.dot(X_test, self.coef_)+self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca574d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = GDRegressor(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69cd20f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.589808027703263 [ 43.51355856  -7.69500784 111.61338053  96.51157875  30.66490076\n",
      "  10.2854373  -58.15333967  56.6532078  124.34330544  65.11421485]\n"
     ]
    }
   ],
   "source": [
    "gd.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4986750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.90197654519747 [  -7.41032246 -200.16617472  533.25518552  338.23799406 -137.42989217\n",
      "  -52.86615812 -170.28983465   54.54383886  571.24202059   53.64638953]\n"
     ]
    }
   ],
   "source": [
    "gd = GDRegressor(epochs=1000)\n",
    "gd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c2481f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "241d73d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.443073473332973"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd975cd",
   "metadata": {},
   "source": [
    "### Time required by batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e0e3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40bfe3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.589808027703263 [ 43.51355856  -7.69500784 111.61338053  96.51157875  30.66490076\n",
      "  10.2854373  -58.15333967  56.6532078  124.34330544  65.11421485]\n",
      "Time take is  0.0029134750366210938\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gd = GDRegressor(epochs=10)\n",
    "gd.fit(X_train,y_train)\n",
    "print(\"Time take is \", time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
